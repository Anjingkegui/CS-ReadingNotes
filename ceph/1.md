     General usage:
     ==============
    usage: ceph [-h] [-c CEPHCONF] [-i INPUT_FILE] [-o OUTPUT_FILE]
                [--setuser SETUSER] [--setgroup SETGROUP] [--id CLIENT_ID]
                [--name CLIENT_NAME] [--cluster CLUSTER]
                [--admin-daemon ADMIN_SOCKET] [-s] [-w] [--watch-debug]
                [--watch-info] [--watch-sec] [--watch-warn] [--watch-error]
                [--watch-channel WATCH_CHANNEL] [--version] [--verbose]
                [--concise] [-f {json,json-pretty,xml,xml-pretty,plain}]
                [--connect-timeout CLUSTER_TIMEOUT]

    Ceph administration tool

    optional arguments:
      -h, --help            request mon help
      -c CEPHCONF, --conf CEPHCONF
                            ceph configuration file
      -i INPUT_FILE, --in-file INPUT_FILE
                            input file, or "-" for stdin
      -o OUTPUT_FILE, --out-file OUTPUT_FILE
                            output file, or "-" for stdout
      --setuser SETUSER     set user file permission
      --setgroup SETGROUP   set group file permission
      --id CLIENT_ID, --user CLIENT_ID
                            client id for authentication
      --name CLIENT_NAME, -n CLIENT_NAME
                            client name for authentication
      --cluster CLUSTER     cluster name
      --admin-daemon ADMIN_SOCKET
                            submit admin-socket commands ("help" for help
      -s, --status          show cluster status
      -w, --watch           watch live cluster changes
      --watch-debug         watch debug events
      --watch-info          watch info events
      --watch-sec           watch security events
      --watch-warn          watch warn events
      --watch-error         watch error events
      --watch-channel WATCH_CHANNEL
                            which log channel to follow when using -w/--watch. One
                            of ['cluster', 'audit', '*'
      --version, -v         display version
      --verbose             make verbose
      --concise             make less verbose
      -f {json,json-pretty,xml,xml-pretty,plain}, --format {json,json-pretty,xml,xml-pretty,plain}
      --connect-timeout CLUSTER_TIMEOUT
                            set a timeout for connecting to the cluster

     Local commands:
     ===============

    ping <mon.id>           Send simple presence/life test to a mon
                            <mon.id> may be 'mon.*' for all mons
    daemon {type.id|path} <cmd>
                            Same as --admin-daemon, but auto-find admin socket
    daemonperf {type.id | path} [stat-pats] [priority] [<interval>] [<count>]
    daemonperf {type.id | path} list|ls [stat-pats] [priority]
                            Get selected perf stats from daemon/admin socket
                            Optional shell-glob comma-delim match string stat-pats
                            Optional selection priority (can abbreviate name):
                             critical, interesting, useful, noninteresting, debug
                            List shows a table of all available stats
                            Run <count> times (default forever),
                             once per <interval> seconds (default 1)


     Monitor commands:
     =================
    auth add <entity> {<caps> [<caps>...]}                                                                  add auth info for <entity> from input file, or random key if no input is given, and/or any caps
                                                                                                             specified in the command
    auth caps <entity> <caps> [<caps>...]                                                                   update caps for <name> from caps specified in the command
    auth export {<entity>}                                                                                  write keyring for requested entity, or master keyring if none given
    auth get <entity>                                                                                       write keyring file with requested key
    auth get-key <entity>                                                                                   display requested key
    auth get-or-create <entity> {<caps> [<caps>...]}                                                        add auth info for <entity> from input file, or random key if no input given, and/or any caps specified
                                                                                                             in the command
    auth get-or-create-key <entity> {<caps> [<caps>...]}                                                    get, or add, key for <name> from system/caps pairs specified in the command.  If key already exists,
                                                                                                             any given caps must match the existing caps for that key.
    auth import                                                                                             auth import: read keyring file from -i <file>
    auth ls                                                                                                 list authentication state
    auth print-key <entity>                                                                                 display requested key
    auth print_key <entity>                                                                                 display requested key
    auth rm <entity>                                                                                        remove all caps for <name>
    balancer dump <plan>                                                                                    Show an optimization plan
    balancer eval {<option>}                                                                                Evaluate data distribution for the current cluster or specific pool or specific plan
    balancer eval-verbose {<option>}                                                                        Evaluate data distribution for the current cluster or specific pool or specific plan (verbosely)
    balancer execute <plan>                                                                                 Execute an optimization plan
    balancer ls                                                                                             List all plans
    balancer mode none|crush-compat|upmap                                                                   Set balancer mode
    balancer off                                                                                            Disable automatic balancing
    balancer on                                                                                             Enable automatic balancing
    balancer optimize <plan> {<pools> [<pools>...]}                                                         Run optimizer to create a new plan
    balancer pool add <pools> [<pools>...]                                                                  Enable automatic balancing for specific pools
    balancer pool ls                                                                                        List automatic balancing pools. Note that empty list means all existing pools will be automatic
                                                                                                             balancing targets, which is the default behaviour of balancer.
    balancer pool rm <pools> [<pools>...]                                                                   Disable automatic balancing for specific pools
    balancer reset                                                                                          Discard all optimization plans
    balancer rm <plan>                                                                                      Discard an optimization plan
    balancer show <plan>                                                                                    Show details of an optimization plan
    balancer status                                                                                         Show balancer status
    config assimilate-conf                                                                                  Assimilate options from a conf, and return a new, minimal conf file
    config dump                                                                                             Show all configuration option(s)
    config generate-minimal-conf                                                                            Generate a minimal ceph.conf file
    config get <who> {<key>}                                                                                Show configuration option(s) for an entity
    config help <key>                                                                                       Describe a configuration option
    config log {<int>}                                                                                      Show recent history of config changes
    config ls                                                                                               List available configuration options
    config reset <int>                                                                                      Revert configuration to previous state
    config rm <who> <name>                                                                                  Clear a configuration option for one or more entities
    config set <who> <name> <value> {--force}                                                               Set a configuration option for one or more entities
    config show <who> {<key>}                                                                               Show running configuration
    config show-with-defaults <who>                                                                         Show running configuration (including compiled-in defaults)
    config-key dump {<key>}                                                                                 dump keys and values (with optional prefix)
    config-key exists <key>                                                                                 check for <key>'s existence
    config-key get <key>                                                                                    get <key>
    config-key ls                                                                                           list keys
    config-key rm <key>                                                                                     rm <key>
    config-key set <key> {<val>}                                                                            set <key> to value <val>
    crash info <id>                                                                                         show crash dump metadata
    crash json_report <hours>                                                                               Crashes in the last <hours> hours
    crash ls                                                                                                Show saved crash dumps
    crash post                                                                                              Add a crash dump (use -i <jsonfile>)
    crash prune <keep>                                                                                      Remove crashes older than <keep> days
    crash rm <id>                                                                                           Remove a saved crash <id>
    crash stat                                                                                              Summarize recorded crashes
    deepsea config-set <key> <value>                                                                        Set a configuration value
    deepsea config-show                                                                                     Show current configuration
    device check-health                                                                                     Check life expectancy of devices
    device get-health-metrics <devid> {<sample>}                                                            Show stored device metrics for the device
    device info <devid>                                                                                     Show information about a device
    device ls                                                                                               Show devices
    device ls-by-daemon <who>                                                                               Show devices associated with a daemon
    device ls-by-host <host>                                                                                Show devices on a host
    device monitoring off                                                                                   Disable device health monitoring
    device monitoring on                                                                                    Enable device health monitoring
    device predict-life-expectancy {<devid>}                                                                Predict life expectancy with local predictor
    device query-daemon-health-metrics <who>                                                                Get device health metrics for a given daemon
    device rm-life-expectancy <devid>                                                                       Clear predicted device life expectancy
    device scrape-daemon-health-metrics <who>                                                               Scrape and store device health metrics for a given daemon
    device scrape-health-metrics {<devid>}                                                                  Scrape and store health metrics
    device set-life-expectancy <devid> <from> {<to>}                                                        Set predicted device life expectancy
    df {detail}                                                                                             show cluster free space stats
    features                                                                                                report of connected features
    fs add_data_pool <fs_name> <pool>                                                                       add data pool <pool>
    fs authorize <filesystem> <entity> <caps> [<caps>...]                                                   add auth for <entity> to access file system <filesystem> based on following directory and permissions
                                                                                                             pairs
    fs dump {<int[0-]>}                                                                                     dump all CephFS status, optionally from epoch
    fs fail <fs_name>                                                                                       bring the file system down and all of its ranks
    fs flag set enable_multiple <val> {--yes-i-really-mean-it}                                              Set a global CephFS flag
    fs get <fs_name>                                                                                        get info about one filesystem
    fs ls                                                                                                   list filesystems
    fs new <fs_name> <metadata> <data> {--force} {--allow-dangerous-metadata-overlay}                       make new filesystem using named pools <metadata> and <data>
    fs reset <fs_name> {--yes-i-really-mean-it}                                                             disaster recovery only: reset to a single-MDS map
    fs rm <fs_name> {--yes-i-really-mean-it}                                                                disable the named filesystem
    fs rm_data_pool <fs_name> <pool>                                                                        remove data pool <pool>
    fs set <fs_name> max_mds|max_file_size|allow_new_snaps|inline_data|cluster_down|allow_dirfrags|         set fs parameter <var> to <val>
     balancer|standby_count_wanted|session_timeout|session_autoclose|allow_standby_replay|down|joinable|
     min_compat_client <val> {--yes-i-really-mean-it}
    fs set-default <fs_name>                                                                                set the default to the named filesystem
    fs status {<fs>}                                                                                        Show the status of a CephFS filesystem
    fs subvolume create <vol_name> <sub_name> {<size>}                                                      Create a CephFS subvolume within an existing volume
    fs subvolume rm <vol_name> <sub_name>                                                                   Delete a CephFS subvolume
    fs volume create <name> {<size>}                                                                        Create a CephFS volume
    fs volume ls                                                                                            List volumes
    fs volume rm <vol_name>                                                                                 Delete a CephFS volume
    fsid                                                                                                    show cluster FSID/UUID
    health {detail}                                                                                         show cluster health
    heap dump|start_profiler|stop_profiler|release|stats                                                    show heap usage info (available only if compiled with tcmalloc)
    influx config-set <key> <value>                                                                         Set a configuration value
    influx config-show                                                                                      Show current configuration
    influx send                                                                                             Force sending data to Influx
    injectargs <injected_args> [<injected_args>...]                                                         inject config arguments into monitor
    insights                                                                                                Retrieve insights report
    insights prune-health <hours>                                                                           Remove health history older than <hours> hours
    iostat                                                                                                  Get IO rates
    log <logtext> [<logtext>...]                                                                            log supplied text to the monitor log
    log last {<int[1-]>} {debug|info|sec|warn|error} {*|cluster|audit}                                      print last few lines of the cluster log
    mds compat rm_compat <int[0-]>                                                                          remove compatible feature
    mds compat rm_incompat <int[0-]>                                                                        remove incompatible feature
    mds compat show                                                                                         show mds compatibility settings
    mds count-metadata <property>                                                                           count MDSs by metadata field property
    mds fail <role_or_gid>                                                                                  Mark MDS failed: trigger a failover if a standby is available
    mds freeze <role_or_gid> <val>                                                                          freeze MDS yes/no
    mds metadata {<who>}                                                                                    fetch metadata for mds <role>
    mds ok-to-stop <ids> [<ids>...]                                                                         check whether stopping the specified MDS would reduce immediate availability
    mds repaired <role>                                                                                     mark a damaged MDS rank as no longer damaged
    mds rm <int[0-]>                                                                                        remove nonactive mds
    mds rmfailed <role> {--yes-i-really-mean-it}                                                            remove failed rank
    mds set_state <int[0-]> <int[0-20]>                                                                     set mds state of <gid> to <numeric-state>
    mds stat                                                                                                show MDS status
    mds versions                                                                                            check running versions of MDSs
    mgr count-metadata <property>                                                                           count ceph-mgr daemons by metadata field property
    mgr dump {<int[0-]>}                                                                                    dump the latest MgrMap
    mgr fail <who>                                                                                          treat the named manager daemon as failed
    mgr metadata {<who>}                                                                                    dump metadata for all daemons or a specific daemon
    mgr module disable <module>                                                                             disable mgr module
    mgr module enable <module> {--force}                                                                    enable mgr module
    mgr module ls                                                                                           list active mgr modules
    mgr self-test background start <workload>                                                               Activate a background workload (one of command_spam, throw_exception)
    mgr self-test background stop                                                                           Stop background workload if any is running
    mgr self-test cluster-log <channel> <priority> <message>                                                Create an audit log record.
    mgr self-test config get <key>                                                                          Peek at a configuration value
    mgr self-test config get_localized <key>                                                                Peek at a configuration value (localized variant)
    mgr self-test health clear {<checks> [<checks>...]}                                                     Clear health checks by name. If no names provided, clear all.
    mgr self-test health set <checks>                                                                       Set a health check from a JSON-formatted description.
    mgr self-test insights_set_now_offset <hours>                                                           Set the now time for the insights module.
    mgr self-test module <module>                                                                           Run another module's self_test() method
    mgr self-test remote                                                                                    Test inter-module calls
    mgr self-test run                                                                                       Run mgr python interface tests
    mgr services                                                                                            list service endpoints provided by mgr modules
    mgr versions                                                                                            check running versions of ceph-mgr daemons
    mon add <name> <IPaddr[:port]>                                                                          add new monitor named <name> at <addr>
    mon compact                                                                                             cause compaction of monitor's leveldb/rocksdb storage
    mon count-metadata <property>                                                                           count mons by metadata field property
    mon dump {<int[0-]>}                                                                                    dump formatted monmap (optionally from epoch)
    mon enable-msgr2                                                                                        enable the msgr2 protocol on port 3300
    mon feature ls {--with-value}                                                                           list available mon map features to be set/unset
    mon feature set <feature_name> {--yes-i-really-mean-it}                                                 set provided feature on mon map
    mon getmap {<int[0-]>}                                                                                  get monmap
    mon metadata {<id>}                                                                                     fetch metadata for mon <id>
    mon ok-to-add-offline                                                                                   check whether adding a mon and not starting it would break quorum
    mon ok-to-rm <id>                                                                                       check whether removing the specified mon would break quorum
    mon ok-to-stop <ids> [<ids>...]                                                                         check whether mon(s) can be safely stopped without reducing immediate availability
    mon rm <name>                                                                                           remove monitor named <name>
    mon scrub                                                                                               scrub the monitor stores
    mon set-addrs <name> <addrs>                                                                            set the addrs (IPs and ports) a specific monitor binds to
    mon set-rank <name> <int>                                                                               set the rank for the specified mon
    mon stat                                                                                                summarize monitor status
    mon sync force {--yes-i-really-mean-it} {--i-know-what-i-am-doing}                                      force sync of and clear monitor store
    mon versions                                                                                            check running versions of monitors
    mon_status                                                                                              report status of monitors
    node ls {all|osd|mon|mds|mgr}                                                                           list all nodes in cluster [type]
    orchestrator device ls {<host> [<host>...]} {json|plain} {--refresh}                                    List devices on a node
    orchestrator host add {<host>}                                                                          Add a host
    orchestrator host ls                                                                                    List hosts
    orchestrator host rm {<host>}                                                                           Remove a host
    orchestrator mds add <svc_arg>                                                                          Create an MDS service
    orchestrator mds rm <svc_id>                                                                            Remove an MDS service
    orchestrator mgr update {<int>} {<hosts> [<hosts>...]}                                                  Update the number of manager instances
    orchestrator mon update {<int>} {<hosts> [<hosts>...]}                                                  Update the number of monitor instances
    orchestrator nfs add <svc_arg> <pool> {<namespace>}                                                     Create an NFS service
    orchestrator nfs rm <svc_id>                                                                            Remove an NFS service
    orchestrator nfs update <svc_id> <int>                                                                  Scale an NFS service
    orchestrator osd create {<svc_arg>}                                                                     Create an OSD service. Either --svc_arg=host:drives or -i <drive_group>
    orchestrator osd rm <svc_id> [<svc_id>...]                                                              Remove OSD services
    orchestrator rgw add <svc_arg>                                                                          Create an RGW service
    orchestrator rgw rm <svc_id>                                                                            Remove an RGW service
    orchestrator service ls {<host>} {mon|mgr|osd|mds|nfs|rgw|rbd-mirror} {<svc_id>} {json|plain}           List services known to orchestrator
    orchestrator service start|stop|reload <svc_type> <svc_name>                                            Start, stop or reload an entire service (i.e. all daemons)
    orchestrator service-instance start|stop|reload <svc_type> <svc_id>                                     Start, stop or reload a specific service instance
    orchestrator set backend {<module_name>}                                                                Select orchestrator module backend
    orchestrator status                                                                                     Report configured backend and its status
    osd add-nodown <ids> [<ids>...]                                                                         mark osd(s) <id> [<id>...] as nodown, or use <all|any> to mark all osds as nodown
    osd add-noin <ids> [<ids>...]                                                                           mark osd(s) <id> [<id>...] as noin, or use <all|any> to mark all osds as noin
    osd add-noout <ids> [<ids>...]                                                                          mark osd(s) <id> [<id>...] as noout, or use <all|any> to mark all osds as noout
    osd add-noup <ids> [<ids>...]                                                                           mark osd(s) <id> [<id>...] as noup, or use <all|any> to mark all osds as noup
    osd blacklist add|rm <EntityAddr> {<float[0.0-]>}                                                       add (optionally until <expire> seconds from now) or remove <addr> from blacklist
    osd blacklist clear                                                                                     clear all blacklisted clients
    osd blacklist ls                                                                                        show blacklisted clients
    osd blocked-by                                                                                          print histogram of which OSDs are blocking their peers
    osd count-metadata <property>                                                                           count OSDs by metadata field property
    osd crush add <osdname (id|osd.id)> <float[0.0-]> <args> [<args>...]                                    add or update crushmap position and weight for <name> with <weight> and location <args>
    osd crush add-bucket <name> <type> {<args> [<args>...]}                                                 add no-parent (probably root) crush bucket <name> of type <type> to location <args>
    osd crush class create <class>                                                                          create crush device class <class>
    osd crush class ls                                                                                      list all crush device classes
    osd crush class ls-osd <class>                                                                          list all osds belonging to the specific <class>
    osd crush class rename <srcname> <dstname>                                                              rename crush device class <srcname> to <dstname>
    osd crush class rm <class>                                                                              remove crush device class <class>
    osd crush create-or-move <osdname (id|osd.id)> <float[0.0-]> <args> [<args>...]                         create entry or move existing entry for <name> <weight> at/to location <args>
    osd crush dump                                                                                          dump crush map
    osd crush get-device-class <ids> [<ids>...]                                                             get classes of specified osd(s) <id> [<id>...]
    osd crush get-tunable straw_calc_version                                                                get crush tunable <tunable>
    osd crush link <name> <args> [<args>...]                                                                link existing entry for <name> under location <args>
    osd crush ls <node>                                                                                     list items beneath a node in the CRUSH tree
    osd crush move <name> <args> [<args>...]                                                                move existing entry for <name> to location <args>
    osd crush rename-bucket <srcname> <dstname>                                                             rename bucket <srcname> to <dstname>
    osd crush reweight <name> <float[0.0-]>                                                                 change <name>'s weight to <weight> in crush map
    osd crush reweight-all                                                                                  recalculate the weights for the tree to ensure they sum correctly
    osd crush reweight-subtree <name> <float[0.0-]>                                                         change all leaf items beneath <name> to <weight> in crush map
    osd crush rm <name> {<ancestor>}                                                                        remove <name> from crush map (everywhere, or just at <ancestor>)
    osd crush rm-device-class <ids> [<ids>...]                                                              remove class of the osd(s) <id> [<id>...],or use <all|any> to remove all.
    osd crush rule create-erasure <name> {<profile>}                                                        create crush rule <name> for erasure coded pool created with <profile> (default default)
    osd crush rule create-replicated <name> <root> <type> {<class>}                                         create crush rule <name> for replicated pool to start from <root>, replicate across buckets of type
                                                                                                             <type>, use devices of type <class> (ssd or hdd)
    osd crush rule create-simple <name> <root> <type> {firstn|indep}                                        create crush rule <name> to start from <root>, replicate across buckets of type <type>, using a choose
                                                                                                             mode of <firstn|indep> (default firstn; indep best for erasure pools)
    osd crush rule dump {<name>}                                                                            dump crush rule <name> (default all)
    osd crush rule ls                                                                                       list crush rules
    osd crush rule ls-by-class <class>                                                                      list all crush rules that reference the same <class>
    osd crush rule rename <srcname> <dstname>                                                               rename crush rule <srcname> to <dstname>
    osd crush rule rm <name>                                                                                remove crush rule <name>
    osd crush set <osdname (id|osd.id)> <float[0.0-]> <args> [<args>...]                                    update crushmap position and weight for <name> to <weight> with location <args>
    osd crush set {<int>}                                                                                   set crush map from input file
    osd crush set-all-straw-buckets-to-straw2                                                               convert all CRUSH current straw buckets to use the straw2 algorithm
    osd crush set-device-class <class> <ids> [<ids>...]                                                     set the <class> of the osd(s) <id> [<id>...],or use <all|any> to set all.
    osd crush set-tunable straw_calc_version <int>                                                          set crush tunable <tunable> to <value>
    osd crush show-tunables                                                                                 show current crush tunables
    osd crush swap-bucket <source> <dest> {--yes-i-really-mean-it}                                          swap existing bucket contents from (orphan) bucket <source> and <target>
    osd crush tree {--show-shadow}                                                                          dump crush buckets and items in a tree view
    osd crush tunables legacy|argonaut|bobtail|firefly|hammer|jewel|optimal|default                         set crush tunables values to <profile>
    osd crush unlink <name> {<ancestor>}                                                                    unlink <name> from crush map (everywhere, or just at <ancestor>)
    osd crush weight-set create <poolname> flat|positional                                                  create a weight-set for a given pool
    osd crush weight-set create-compat                                                                      create a default backward-compatible weight-set
    osd crush weight-set dump                                                                               dump crush weight sets
    osd crush weight-set ls                                                                                 list crush weight sets
    osd crush weight-set reweight <poolname> <item> <float[0.0-]> [<float[0.0-]>...]                        set weight for an item (bucket or osd) in a pool's weight-set
    osd crush weight-set reweight-compat <item> <float[0.0-]> [<float[0.0-]>...]                            set weight for an item (bucket or osd) in the backward-compatible weight-set
    osd crush weight-set rm <poolname>                                                                      remove the weight-set for a given pool
    osd crush weight-set rm-compat                                                                          remove the backward-compatible weight-set
    osd deep-scrub <who>                                                                                    initiate deep scrub on osd <who>, or use <all|any> to deep scrub all
    osd destroy <osdname (id|osd.id)> {--force} {--yes-i-really-mean-it}                                    mark osd as being destroyed. Keeps the ID intact (allowing reuse), but removes cephx keys, config-key
                                                                                                             data and lockbox keys, rendering data permanently unreadable.
    osd destroy-actual <osdname (id|osd.id)> {--yes-i-really-mean-it}                                       mark osd as being destroyed. Keeps the ID intact (allowing reuse), but removes cephx keys, config-key
                                                                                                             data and lockbox keys, rendering data permanently unreadable.
    osd df {plain|tree} {class|name} {<filter>}                                                             show OSD utilization
    osd down <ids> [<ids>...]                                                                               set osd(s) <id> [<id>...] down, or use <any|all> to set all osds down
    osd dump {<int[0-]>}                                                                                    print summary of OSD map
    osd erasure-code-profile get <name>                                                                     get erasure code profile <name>
    osd erasure-code-profile ls                                                                             list all erasure code profiles
    osd erasure-code-profile rm <name>                                                                      remove erasure code profile <name>
    osd erasure-code-profile set <name> {<profile> [<profile>...]} {--force}                                create erasure code profile <name> with [<key[=value]> ...] pairs. Add a --force at the end to override
                                                                                                             an existing profile (VERY DANGEROUS)
    osd find <osdname (id|osd.id)>                                                                          find osd <id> in the CRUSH map and show its location
    osd force-create-pg <pgid> {--yes-i-really-mean-it}                                                     force creation of pg <pgid>
    osd get-require-min-compat-client                                                                       get the minimum client version we will maintain compatibility with
    osd getcrushmap {<int[0-]>}                                                                             get CRUSH map
    osd getmap {<int[0-]>}                                                                                  get OSD map
    osd getmaxosd                                                                                           show largest OSD id
    osd in <ids> [<ids>...]                                                                                 set osd(s) <id> [<id>...] in, can use <any|all> to automatically set all previously out osds in
    osd last-stat-seq <osdname (id|osd.id)>                                                                 get the last pg stats sequence number reported for this osd
    osd lost <osdname (id|osd.id)> {--yes-i-really-mean-it}                                                 mark osd as permanently lost. THIS DESTROYS DATA IF NO MORE REPLICAS EXIST, BE CAREFUL
    osd ls {<int[0-]>}                                                                                      show all OSD ids
    osd ls-tree {<int[0-]>} {<name>}                                                                        show OSD ids under bucket <name> in the CRUSH map
    osd map <poolname> <objectname> {<nspace>}                                                              find pg for <object> in <pool> with [namespace]
    osd metadata {<osdname (id|osd.id)>}                                                                    fetch metadata for osd {id} (default all)
    osd new {<uuid>} {<osdname (id|osd.id)>}                                                                Create a new OSD. If supplied, the `id` to be replaced needs to exist and have been previously
                                                                                                             destroyed. Reads secrets from JSON file via `-i <file>` (see man page).
    osd numa-status                                                                                         show NUMA status of OSDs
    osd ok-to-stop <ids> [<ids>...]                                                                         check whether osd(s) can be safely stopped without reducing immediate data availability
    osd out <ids> [<ids>...]                                                                                set osd(s) <id> [<id>...] out, or use <any|all> to set all osds out
    osd pause                                                                                               pause osd
    osd perf                                                                                                print dump of OSD perf summary stats
    osd pg-temp <pgid> {<osdname (id|osd.id)> [<osdname (id|osd.id)>...]}                                   set pg_temp mapping pgid:[<id> [<id>...]] (developers only)
    osd pg-upmap <pgid> <osdname (id|osd.id)> [<osdname (id|osd.id)>...]                                    set pg_upmap mapping <pgid>:[<id> [<id>...]] (developers only)
    osd pg-upmap-items <pgid> <osdname (id|osd.id)> [<osdname (id|osd.id)>...]                              set pg_upmap_items mapping <pgid>:{<id> to <id>, [...]} (developers only)
    osd pool application disable <poolname> <app> {--yes-i-really-mean-it}                                  disables use of an application <app> on pool <poolname>
    osd pool application enable <poolname> <app> {--yes-i-really-mean-it}                                   enable use of an application <app> [cephfs,rbd,rgw] on pool <poolname>
    osd pool application get {<poolname>} {<app>} {<key>}                                                   get value of key <key> of application <app> on pool <poolname>
    osd pool application rm <poolname> <app> <key>                                                          removes application <app> metadata key <key> on pool <poolname>
    osd pool application set <poolname> <app> <key> <value>                                                 sets application <app> metadata key <key> to <value> on pool <poolname>
    osd pool autoscale-status                                                                               report on pool pg_num sizing recommendation and intent
    osd pool cancel-force-backfill <poolname> [<poolname>...]                                               restore normal recovery priority of specified pool <who>
    osd pool cancel-force-recovery <poolname> [<poolname>...]                                               restore normal recovery priority of specified pool <who>
    osd pool create <poolname> <int[0-]> {<int[0-]>} {replicated|erasure} {<erasure_code_profile>}          create pool
     {<rule>} {<int>} {<int>} {<int[0-]>} {<int[0-]>} {<float[0.0-1.0]>}
    osd pool deep-scrub <poolname> [<poolname>...]                                                          initiate deep-scrub on pool <who>
    osd pool force-backfill <poolname> [<poolname>...]                                                      force backfill of specified pool <who> first
    osd pool force-recovery <poolname> [<poolname>...]                                                      force recovery of specified pool <who> first
    osd pool get <poolname> size|min_size|pg_num|pgp_num|crush_rule|hashpspool|nodelete|nopgchange|         get pool parameter <var>
     nosizechange|write_fadvise_dontneed|noscrub|nodeep-scrub|hit_set_type|hit_set_period|hit_set_count|
     hit_set_fpp|use_gmt_hitset|target_max_objects|target_max_bytes|cache_target_dirty_ratio|cache_target_
     dirty_high_ratio|cache_target_full_ratio|cache_min_flush_age|cache_min_evict_age|erasure_code_profile|
     min_read_recency_for_promote|all|min_write_recency_for_promote|fast_read|hit_set_grade_decay_rate|hit_
     set_search_last_n|scrub_min_interval|scrub_max_interval|deep_scrub_interval|recovery_priority|
     recovery_op_priority|scrub_priority|compression_mode|compression_algorithm|compression_required_ratio|
     compression_max_blob_size|compression_min_blob_size|csum_type|csum_min_block|csum_max_block|allow_ec_
     overwrites|fingerprint_algorithm|pg_autoscale_mode|pg_autoscale_bias|pg_num_min|target_size_bytes|
     target_size_ratio
    osd pool get-quota <poolname>                                                                           obtain object or byte limits for pool
    osd pool ls {detail}                                                                                    list pools
    osd pool mksnap <poolname> <snap>                                                                       make snapshot <snap> in <pool>
    osd pool rename <poolname> <poolname>                                                                   rename <srcpool> to <destpool>
    osd pool repair <poolname> [<poolname>...]                                                              initiate repair on pool <who>
    osd pool rm <poolname> {<poolname>} {--yes-i-really-really-mean-it} {--yes-i-really-really-mean-it-not- remove pool
     faking}
    osd pool rmsnap <poolname> <snap>                                                                       remove snapshot <snap> from <pool>
    osd pool scrub <poolname> [<poolname>...]                                                               initiate scrub on pool <who>
    osd pool set <poolname> size|min_size|pg_num|pgp_num|pgp_num_actual|crush_rule|hashpspool|nodelete|     set pool parameter <var> to <val>
     nopgchange|nosizechange|write_fadvise_dontneed|noscrub|nodeep-scrub|hit_set_type|hit_set_period|hit_
     set_count|hit_set_fpp|use_gmt_hitset|target_max_bytes|target_max_objects|cache_target_dirty_ratio|
     cache_target_dirty_high_ratio|cache_target_full_ratio|cache_min_flush_age|cache_min_evict_age|min_
     read_recency_for_promote|min_write_recency_for_promote|fast_read|hit_set_grade_decay_rate|hit_set_
     search_last_n|scrub_min_interval|scrub_max_interval|deep_scrub_interval|recovery_priority|recovery_op_
     priority|scrub_priority|compression_mode|compression_algorithm|compression_required_ratio|compression_
     max_blob_size|compression_min_blob_size|csum_type|csum_min_block|csum_max_block|allow_ec_overwrites|
     fingerprint_algorithm|pg_autoscale_mode|pg_autoscale_bias|pg_num_min|target_size_bytes|target_size_
     ratio <val> {--yes-i-really-mean-it}
    osd pool set-quota <poolname> max_objects|max_bytes <val>                                               set object or byte limit on pool
    osd pool stats {<poolname>}                                                                             obtain stats from all pools, or from specified pool
    osd primary-affinity <osdname (id|osd.id)> <float[0.0-1.0]>                                             adjust osd primary-affinity from 0.0 <= <weight> <= 1.0
    osd primary-temp <pgid> <osdname (id|osd.id)>                                                           set primary_temp mapping pgid:<id>|-1 (developers only)
    osd purge <osdname (id|osd.id)> {--force} {--yes-i-really-mean-it}                                      purge all osd data from the monitors including the OSD id and CRUSH position
    osd purge-actual <osdname (id|osd.id)> {--yes-i-really-mean-it}                                         purge all osd data from the monitors. Combines `osd destroy`, `osd rm`, and `osd crush rm`.
    osd purge-new <osdname (id|osd.id)> {--yes-i-really-mean-it}                                            purge all traces of an OSD that was partially created but never started
    osd repair <who>                                                                                        initiate repair on osd <who>, or use <all|any> to repair all
    osd require-osd-release luminous|mimic|nautilus {--yes-i-really-mean-it}                                set the minimum allowed OSD release to participate in the cluster
    osd reweight <osdname (id|osd.id)> <float[0.0-1.0]>                                                     reweight osd to 0.0 < <weight> < 1.0
    osd reweight-by-pg {<int>} {<float>} {<int>} {<poolname> [<poolname>...]}                               reweight OSDs by PG distribution [overload-percentage-for-consideration, default 120]
    osd reweight-by-utilization {<int>} {<float>} {<int>} {--no-increasing}                                 reweight OSDs by utilization [overload-percentage-for-consideration, default 120]
    osd reweightn <weights>                                                                                 reweight osds with {<id>: <weight>,...})
    osd rm-nodown <ids> [<ids>...]                                                                          allow osd(s) <id> [<id>...] to be marked down (if they are currently marked as nodown), can use <all|
                                                                                                             any> to automatically filter out all nodown osds
    osd rm-noin <ids> [<ids>...]                                                                            allow osd(s) <id> [<id>...] to be marked in (if they are currently marked as noin), can use <all|any>
                                                                                                             to automatically filter out all noin osds
    osd rm-noout <ids> [<ids>...]                                                                           allow osd(s) <id> [<id>...] to be marked out (if they are currently marked as noout), can use <all|any>
                                                                                                             to automatically filter out all noout osds
    osd rm-noup <ids> [<ids>...]                                                                            allow osd(s) <id> [<id>...] to be marked up (if they are currently marked as noup), can use <all|any>
                                                                                                             to automatically filter out all noup osds
    osd rm-pg-upmap <pgid>                                                                                  clear pg_upmap mapping for <pgid> (developers only)
    osd rm-pg-upmap-items <pgid>                                                                            clear pg_upmap_items mapping for <pgid> (developers only)
    osd safe-to-destroy <ids> [<ids>...]                                                                    check whether osd(s) can be safely destroyed without reducing data durability
    osd scrub <who>                                                                                         initiate scrub on osd <who>, or use <all|any> to scrub all
    osd set full|pause|noup|nodown|noout|noin|nobackfill|norebalance|norecover|noscrub|nodeep-scrub|        set <key>
     notieragent|nosnaptrim|pglog_hardlimit {--yes-i-really-mean-it}
    osd set-backfillfull-ratio <float[0.0-1.0]>                                                             set usage ratio at which OSDs are marked too full to backfill
    osd set-full-ratio <float[0.0-1.0]>                                                                     set usage ratio at which OSDs are marked full
    osd set-nearfull-ratio <float[0.0-1.0]>                                                                 set usage ratio at which OSDs are marked near-full
    osd set-require-min-compat-client <version> {--yes-i-really-mean-it}                                    set the minimum client version we will maintain compatibility with
    osd setcrushmap {<int>}                                                                                 set crush map from input file
    osd setmaxosd <int[0-]>                                                                                 set new maximum osd value
    osd stat                                                                                                print summary of OSD map
    osd status {<bucket>}                                                                                   Show the status of OSDs within a bucket, or all
    osd test-reweight-by-pg {<int>} {<float>} {<int>} {<poolname> [<poolname>...]}                          dry run of reweight OSDs by PG distribution [overload-percentage-for-consideration, default 120]
    osd test-reweight-by-utilization {<int>} {<float>} {<int>} {--no-increasing}                            dry run of reweight OSDs by utilization [overload-percentage-for-consideration, default 120]
    osd tier add <poolname> <poolname> {--force-nonempty}                                                   add the tier <tierpool> (the second one) to base pool <pool> (the first one)
    osd tier add-cache <poolname> <poolname> <int[0-]>                                                      add a cache <tierpool> (the second one) of size <size> to existing pool <pool> (the first one)
    osd tier cache-mode <poolname> none|writeback|forward|readonly|readforward|proxy|readproxy {--yes-i-    specify the caching mode for cache tier <pool>
     really-mean-it}
    osd tier rm <poolname> <poolname>                                                                       remove the tier <tierpool> (the second one) from base pool <pool> (the first one)
    osd tier rm-overlay <poolname>                                                                          remove the overlay pool for base pool <pool>
    osd tier set-overlay <poolname> <poolname>                                                              set the overlay pool for base pool <pool> to be <overlaypool>
    osd tree {<int[0-]>} {up|down|in|out|destroyed [up|down|in|out|destroyed...]}                           print OSD tree
    osd tree-from {<int[0-]>} <bucket> {up|down|in|out|destroyed [up|down|in|out|destroyed...]}             print OSD tree in bucket
    osd unpause                                                                                             unpause osd
    osd unset full|pause|noup|nodown|noout|noin|nobackfill|norebalance|norecover|noscrub|nodeep-scrub|      unset <key>
     notieragent|nosnaptrim
    osd utilization                                                                                         get basic pg distribution stats
    osd versions                                                                                            check running versions of OSDs
    pg cancel-force-backfill <pgid> [<pgid>...]                                                             restore normal backfill priority of <pgid>
    pg cancel-force-recovery <pgid> [<pgid>...]                                                             restore normal recovery priority of <pgid>
    pg debug unfound_objects_exist|degraded_pgs_exist                                                       show debug info about pgs
    pg deep-scrub <pgid>                                                                                    start deep-scrub on <pgid>
    pg dump {all|summary|sum|delta|pools|osds|pgs|pgs_brief [all|summary|sum|delta|pools|osds|pgs|pgs_      show human-readable versions of pg map (only 'all' valid with plain)
     brief...]}
    pg dump_json {all|summary|sum|pools|osds|pgs [all|summary|sum|pools|osds|pgs...]}                       show human-readable version of pg map in json only
    pg dump_pools_json                                                                                      show pg pools info in json only
    pg dump_stuck {inactive|unclean|stale|undersized|degraded [inactive|unclean|stale|undersized|degraded.. show information about stuck pgs
     .]} {<int>}
    pg force-backfill <pgid> [<pgid>...]                                                                    force backfill of <pgid> first
    pg force-recovery <pgid> [<pgid>...]                                                                    force recovery of <pgid> first
    pg getmap                                                                                               get binary pg map to -o/stdout
    pg ls {<int>} {<states> [<states>...]}                                                                  list pg with specific pool, osd, state
    pg ls-by-osd <osdname (id|osd.id)> {<int>} {<states> [<states>...]}                                     list pg on osd [osd]
    pg ls-by-pool <poolstr> {<states> [<states>...]}                                                        list pg with pool = [poolname]
    pg ls-by-primary <osdname (id|osd.id)> {<int>} {<states> [<states>...]}                                 list pg with primary = [osd]
    pg map <pgid>                                                                                           show mapping of pg to osds
    pg repair <pgid>                                                                                        start repair on <pgid>
    pg repeer <pgid>                                                                                        force a PG to repeer
    pg scrub <pgid>                                                                                         start scrub on <pgid>
    pg stat                                                                                                 show placement group status.
    progress                                                                                                Show progress of recovery operations
    progress clear                                                                                          Reset progress tracking
    progress json                                                                                           Show machine readable progress information
    prometheus file_sd_config                                                                               Return file_sd compatible prometheus config for mgr cluster
    quorum enter|exit                                                                                       enter or exit quorum
    quorum_status                                                                                           report status of monitor quorum
    rbd perf image counters {<pool_spec>} {write_ops|write_bytes|write_latency|read_ops|read_bytes|read_    Retrieve current RBD IO performance counters
     latency}
    rbd perf image stats {<pool_spec>} {write_ops|write_bytes|write_latency|read_ops|read_bytes|read_       Retrieve current RBD IO performance stats
     latency}
    report {<tags> [<tags>...]}                                                                             report full status of cluster, optional title tag strings
    restful create-key <key_name>                                                                           Create an API key with this name
    restful create-self-signed-cert                                                                         Create localized self signed certificate
    restful delete-key <key_name>                                                                           Delete an API key with this name
    restful list-keys                                                                                       List all API keys
    restful restart                                                                                         Restart API server
    service dump                                                                                            dump service map
    service status                                                                                          dump service state
    smart {<devid>}                                                                                         Query health metrics for underlying device
    status                                                                                                  show cluster status
    telegraf config-set <key> <value>                                                                       Set a configuration value
    telegraf config-show                                                                                    Show current configuration
    telegraf send                                                                                           Force sending data to Telegraf
    telemetry off                                                                                           Disable telemetry reports from this cluster
    telemetry on                                                                                            Enable telemetry reports from this cluster
    telemetry send                                                                                          Force sending data to Ceph telemetry
    telemetry show                                                                                          Show last report or report to be sent
    telemetry status                                                                                        Show current configuration
    tell <name (type.id)> <args> [<args>...]                                                                send a command to a specific daemon
    time-sync-status                                                                                        show time sync status
    version                                                                                                 show mon daemon version
    versions                                                                                                check running versions of ceph daemons
    zabbix config-set <key> <value>                                                                         Set a configuration value
    zabbix config-show                                                                                      Show current configuration
    zabbix send                                                                                             Force sending data to Zabbix



